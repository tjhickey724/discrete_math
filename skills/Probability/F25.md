# F25: Probability: Random Variables and Expected Values


* [Skill Description](#skill-description)
* [Sample Problems](#Sample-Problems)
* [Skill Tutorial](#Tutorial)
* [Answers to Sample Problems](#Answers)
* [Further Reading](#Reading)

---


# Skill Definition

The ability to calculate the expected value of some function on a probability space.


---

# Sample-Problems

## Problem 1
Let $S$ be the sample space of rolls of 1 six-sided die and let $R_1(\omega)$ be the
value that is rolled. What is the expected value of $R$ assuming the die is fair?
What if the die is loaded so that "1" appears 50% of the time and the other 5 numbers
appear $10% of the time?
Suppose we are rolling $k$ dice, let $R_k$ be the random variable that sums the values on
the dice. What is the expected value of $R_2$?

## Problem 2
Let $S$ be the sample space of all finite sequences of coin flips.
Let $R(\omega)$ be the position of the first "tails" in the coin flip, so $R(HHHT)=4$
What is the expected value of $R$?


## Problem 3
Let $S$ be the sample space of all craps games, i.e. an outcome is a sequence of dice rolls
that might appear in a game of Craps. Let $R$ be the random variable which counts the length
of the game, i.e. the number of time the dice were rolled.  How would you calculate the
expected value of $R$, i.e. the averate length of a game of craps?




---

# Tutorial


## Random Variables

A **random variable** X on a probability space is a total function whose domain is the sample space.

Since we are assuming the sample space $S$ is either finite or countable infinite, the range of a random
variable is either finite or countably infinite, and the outcomes where $X$ takes a given value can be viewed
as events written as
* ${\rm Pr}(X=k)$

Two random variables $R_1$ and $R_2$ are said to be independent provided these events are all independent, that is

$R_1$ and $R_2$ are said to be **independent random variables** by definition if forall $j$ and $k$ we have
* ${\rm Pr}[R_1=j]$ and ${\rm Pr}[R_2=k]$ are independent events

$$
{\rm Pr}[(R_1=j) \wedge (R_2=k)] = {\rm Pr}[R_1=j] * {\rm Pr}[R_2=k] 
$$

Making the assumption that two random variables are independent makes it easier to reason about them
because knowing the value of one variable doesn't impact the value of the other, that is

* ${\rm Pr}[(R_1=j) \ \vert \ (R_2=k)]$
* $= {\rm Pr}[(R_1=j) \wedge (R_2=k)]/{\rm Pr}[R_2=k]$  by definition of conditional probability
* $= {\rm Pr}[R_1=j] * {\rm Pr}[R_2=k] /{\rm Pr}[R_2=k]$ by definition of independence
* $=  {\rm Pr}[R_1=j]$ by algebra assuming ${\rm Pr}[R_2=k] \not - 0$

If we can reasonably believe that two variables are independent, it makes it much easier to reason about them.


## Expected Value, a generalization of the mean

The **expected value** $E(R)$ of a random varaible $R$ on a probability space with sample space $S$ is defined to be

$$
{\rm E}[R] = \sum_\limits{\omega\in S} R(\omega)\ {\rm Pr}[\omega]
$$


When $S$ is a finite set and every outcome has the same probability, this gives the mean of that set of values!
Expected value is more general than the mean as it allows for non-uniform distributions and even infinite sample
spaces.

We can also define the expected value in terms of larger events than outcomes.

**Theorem** 
The expectation is the weighted average of all values in the range of $R$
where the weights are the probability the variable takes that value, i.e.

$$
{\rm E}[R] = \sum_\limits{k\in R(S)} k\ {\rm Pr}[R=k]
$$



## Applications to Computer Science
A very common application of random variables is to estimate the average esecution time $T$ for a program.
The sample space is the set of all inputs $\omega$, the random variable is the execution time $T(\omega)$ on a particular input,
the events of interest are $E_1,E_2,\ldots$ where $E_k$ is the set of inputs of size $k$ and we want to find
a formula for the expected value of $T$ when we restrict $S$ to the inputs of size $k$, or when we model 
a distribution which occurs in practice, i.e. what is the probablity that the program be give input $\omega$.

One classic example is to compute the averate execution time for sorting algorithms such as quicksort. 
But this also comes up when analyzing the efficiency of a web app where you can model the likelihood of 
particular inputs from the users (e.g. google searches!)



---

# Answers

## Problem 1


---

## Problem 2

